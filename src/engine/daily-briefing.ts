/**
 * Daily Briefing Engine v2 â€” Deep Analysis Blog
 *
 * Pipeline:
 * Phase 1: Collect raw data (Reddit + X.com)
 * Phase 2: Fetch full article content (Jina Reader)
 * Phase 3: LLM deep analysis (Claude Opus â†’ translate + analyze + blog)
 * Phase 4: Generate beautiful é£ä¹¦æ–‡æ¡£ with blog format
 */

import * as lark from '@larksuiteoapi/node-sdk';
import { scanAllSubreddits, searchReddit } from '../channels/reddit-scanner.js';
import { fetchFollowingTimeline } from '../channels/x-feed-reader.js';
import { fetchArticlesBatch, type ArticleContent } from '../utils/article-fetcher.js';
import { generateTopicLibrary } from './topic-generator.js';
import { AntigravityAPI, MODELS } from './antigravity-api.js';
import { GoogleAuth } from '../auth/google-auth.js';
import path from 'path';
import { fileURLToPath } from 'url';
import config from '../../config/config.json' with { type: 'json' };
import sourcesConfig from '../../config/sources.json' with { type: 'json' };

const __dirname = path.dirname(fileURLToPath(import.meta.url));
const PROJECT_ROOT = path.resolve(__dirname, '..', '..');
const DATA_DIR = path.join(PROJECT_ROOT, 'data');

// â”€â”€â”€ Types â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

interface RawItem {
    title: string;
    summary: string;
    url: string;
    score: number;
    source: string;
    fullContent?: string;
    imageUrls?: string[];
}

interface BlogSection {
    heading: string;
    analysis: string;
    sources: Array<{ title: string; url: string }>;
    image_url?: string;
}

interface BlogBriefing {
    title: string;
    date: string;
    intro: string;
    sections: BlogSection[];
    conclusion: string;
    key_insights: string[];
}

// â”€â”€â”€ Feishu Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function getFeishuClient(): lark.Client {
    return new lark.Client({
        appId: config.feishu.appId,
        appSecret: config.feishu.appSecret,
    });
}

async function getFeishuToken(): Promise<string> {
    const res = await fetch('https://open.larksuite.com/open-apis/auth/v3/tenant_access_token/internal', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ app_id: config.feishu.appId, app_secret: config.feishu.appSecret }),
    });
    const json = await res.json() as { tenant_access_token?: string };
    if (!json.tenant_access_token) throw new Error('Failed to get Feishu access token');
    return json.tenant_access_token;
}

// â”€â”€â”€ Phase 1: Data Collection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function collectAllData(): Promise<RawItem[]> {
    const items: RawItem[] = [];

    // --- Reddit: Priority 1 â€” scan core subreddits (config-driven) ---
    try {
        const scanResult = await scanAllSubreddits(undefined, 20);
        for (const p of scanResult.posts.slice(0, 15)) {
            items.push({
                title: p.title,
                summary: p.body.slice(0, 200),
                url: p.url,
                score: p.score,
                source: `Reddit r/${p.subreddit}`,
            });
        }
        console.log(`[BRIEFING] Reddit scan: ${scanResult.posts.length} matches from ${scanResult.subredditsScanned.join(', ')}`);
    } catch (e: any) {
        console.error('[BRIEFING] Reddit scan failed:', e.message);
    }

    // --- Reddit: Priority 3 â€” keyword search across r/all ---
    const searchQueries = [
        // EN core queries
        'ai agent OR agentic ai OR autonomous agent',
        'openclaw OR langchain OR crewai OR autogen',
        'ai saas OR ai startup OR vibe coding',
        'ai ecommerce OR shopify ai OR temu ai',
        // CN queries
        'AIæ™ºèƒ½ä½“ OR å¤§æ¨¡å‹ OR è·¨å¢ƒç”µå•†',
    ];
    for (const query of searchQueries) {
        try {
            const posts = await searchReddit(query, 'all', 10);
            for (const p of posts.slice(0, 5)) {
                items.push({
                    title: p.title,
                    summary: p.body.slice(0, 200),
                    url: p.url,
                    score: p.score,
                    source: `Reddit r/${p.subreddit}`,
                });
            }
            // Rate limit
            await new Promise(r => setTimeout(r, 500));
        } catch (e: any) {
            console.error(`[BRIEFING] Reddit search "${query.slice(0, 30)}" failed:`, e.message);
        }
    }

    // --- X.com: Priority 2 â€” following timeline (includes focus accounts) ---
    try {
        const tweets = await fetchFollowingTimeline(40);
        const meaningful = tweets
            .filter(t => !t.isRetweet || t.quotedTweet)
            .filter(t => t.text.length > 20);
        meaningful.sort((a, b) => (b.likes + b.retweets) - (a.likes + a.retweets));

        for (const t of meaningful.slice(0, 15)) {
            items.push({
                title: `@${t.handle}: ${t.text.slice(0, 80)}`,
                summary: t.text.slice(0, 300),
                url: t.url,
                score: t.likes + t.retweets,
                source: `X.com @${t.handle}`,
                imageUrls: t.mediaUrls,
            });
        }
    } catch (e: any) {
        console.error('[BRIEFING] X.com feed failed:', e.message);
    }

    // Deduplicate by URL
    const seen = new Set<string>();
    const unique = items.filter(i => {
        if (seen.has(i.url)) return false;
        seen.add(i.url);
        return true;
    });

    console.log(`[BRIEFING] Phase 1: Collected ${unique.length} unique items (${items.length} raw)`);
    return unique;
}

// â”€â”€â”€ Phase 2: Full Article Fetch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function enrichWithFullContent(items: RawItem[]): Promise<RawItem[]> {
    // Sort by score and pick top 10 for full-text fetch
    const sorted = [...items].sort((a, b) => b.score - a.score);
    const topItems = sorted.slice(0, 10);

    // Extract URLs that are actual articles (not Reddit self-posts)
    const articleUrls = topItems
        .map(item => item.url)
        .filter(url => url && !url.includes('reddit.com/r/') && !url.includes('x.com'));

    if (articleUrls.length > 0) {
        const articles = await fetchArticlesBatch(articleUrls, 5);
        const articleMap = new Map<string, ArticleContent>();
        for (const a of articles) {
            if (a.content.length > 100) articleMap.set(a.url, a);
        }

        // Enrich items with full content
        for (const item of items) {
            const article = articleMap.get(item.url);
            if (article) {
                item.fullContent = article.content;
                if (article.imageUrls.length > 0) {
                    item.imageUrls = [...(item.imageUrls || []), ...article.imageUrls];
                }
            }
        }
    }

    const enriched = items.filter(i => i.fullContent).length;
    console.log(`[BRIEFING] Phase 2: Enriched ${enriched}/${items.length} items with full content`);
    return items;
}

// â”€â”€â”€ Phase 3: LLM Deep Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function generateBlogAnalysis(items: RawItem[], date: string): Promise<BlogBriefing> {
    const auth = new GoogleAuth(DATA_DIR);
    auth.load();
    const api = new AntigravityAPI(auth);

    if (!api.ready) {
        console.error('[BRIEFING] LLM API not ready, generating basic briefing');
        return generateFallbackBriefing(items, date);
    }

    // Build context for LLM
    const context = items.map((item, i) => {
        let entry = `[${i + 1}] ${item.title}\næ¥æº: ${item.source} | è¯„åˆ†: ${item.score}\nURL: ${item.url}\næ‘˜è¦: ${item.summary}`;
        if (item.fullContent) {
            entry += `\n\nå…¨æ–‡å†…å®¹:\n${item.fullContent}`;
        }
        return entry;
    }).join('\n\n---\n\n');

    const prompt = `ä»¥ä¸‹æ˜¯ä»Šå¤© (${date}) ä» Reddit å’Œ X.com æ”¶é›†çš„ ${items.length} æ¡ AI/Agent é¢†åŸŸå†…å®¹ã€‚

è¯·ä½ ä½œä¸ºä¸“ä¸šç§‘æŠ€åšå®¢ä½œè€…ï¼Œå†™ä¸€ç¯‡æ·±åº¦åˆ†ææ–‡ç« ï¼š

1. **ç¿»è¯‘**ï¼šæ‰€æœ‰è‹±æ–‡å†…å®¹ç¿»è¯‘ä¸ºæµç•…çš„ä¸­æ–‡
2. **åˆ†ç»„**ï¼šæŒ‰ä¸»é¢˜å½’ç±»ï¼ˆ3-5 ä¸ªä»Šæ—¥æ ¸å¿ƒä¸»é¢˜ï¼‰ï¼Œä¸è¦æŒ‰æ¥æºåˆ†ç»„
3. **æ·±åº¦åˆ†æ**ï¼šæ¯ä¸ªä¸»é¢˜å†™ 200-400 å­—çš„æ·±åº¦åˆ†æï¼Œæœ‰è‡ªå·±çš„è§‚ç‚¹
4. **å¼•ç”¨åŸæ–‡**ï¼šå¼•ç”¨å…³é”®å†…å®¹æ—¶æ ‡æ³¨æ¥æº
5. **ä¿ç•™é“¾æ¥**ï¼šæ¯æ¡å†…å®¹ä¿ç•™åŸå§‹ URL
6. **å›¾ç‰‡**ï¼šå¦‚æœåŸå§‹å†…å®¹æœ‰å›¾ç‰‡URLï¼Œæ ‡æ³¨å‡ºæ¥

ä¸¥æ ¼æŒ‰ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼ˆä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ï¼‰ï¼š

{
  "title": "å¸å¼•çœ¼çƒçš„ä¸­æ–‡æ ‡é¢˜",
  "intro": "100-150å­—å¼•è¨€ï¼Œæ¦‚æ‹¬ä»Šæ—¥ä¸»è¦åŠ¨æ€",
  "sections": [
    {
      "heading": "ä¸»é¢˜åç§°ï¼ˆä¸­æ–‡ï¼‰",
      "analysis": "æ·±åº¦åˆ†ææ­£æ–‡ï¼ˆä¸­æ–‡ï¼Œ200-400å­—ï¼Œå¯ä»¥åŒ…å«å¼•ç”¨å’Œé“¾æ¥ï¼‰",
      "sources": [{"title": "æ¥æºæ ‡é¢˜", "url": "https://..."}],
      "image_url": "å›¾ç‰‡URLï¼ˆå¦‚æœ‰ï¼‰"
    }
  ],
  "conclusion": "100-150å­—æ€»ç»“",
  "key_insights": ["æ´å¯Ÿ1", "æ´å¯Ÿ2", "æ´å¯Ÿ3"]
}

åŸå§‹å†…å®¹ï¼š

${context}`;

    try {
        console.log(`[BRIEFING] Phase 3: Sending ${context.length} chars to LLM for analysis...`);
        const response = await api.chat(
            [{ role: 'user', text: prompt }],
            'ä½ æ˜¯é¡¶çº§ç§‘æŠ€åšå®¢ä½œè€…ï¼Œæ“…é•¿ AI/Agent é¢†åŸŸæ·±åº¦åˆ†æã€‚è¾“å‡ºå¿…é¡»æ˜¯çº¯ JSONï¼Œä¸è¦åŠ  markdown ä»£ç å—ã€‚',
            MODELS.taskPrimary,
            MODELS.taskFallback,
        );

        // Parse JSON from response â€” robust extraction
        const blog = extractBlogJson(response, date);
        if (blog) {
            console.log(`[BRIEFING] Phase 3: Generated blog with ${blog.sections?.length ?? 0} sections`);
            return blog;
        }

        console.error('[BRIEFING] Could not parse LLM response, using fallback');
        return generateFallbackBriefing(items, date);
    } catch (e: any) {
        console.error('[BRIEFING] LLM analysis failed:', e.message);
        return generateFallbackBriefing(items, date);
    }
}

/** Robust JSON extraction from LLM response */
function extractBlogJson(response: string, date: string): BlogBriefing | null {
    let text = response.trim();

    // Strip markdown code fence
    text = text.replace(/^```(?:json)?\s*\n?/m, '').replace(/\n?\s*```\s*$/m, '');

    // Try direct parse
    try { const r = JSON.parse(text); r.date = date; return r; } catch { }

    // Extract JSON between first { and last }
    const firstBrace = text.indexOf('{');
    const lastBrace = text.lastIndexOf('}');
    if (firstBrace < 0 || lastBrace <= firstBrace) {
        return extractBlogFromMarkdown(text, date);
    }

    const jsonCandidate = text.slice(firstBrace, lastBrace + 1);

    // Try raw candidate
    try { const r = JSON.parse(jsonCandidate); r.date = date; return r; } catch { }

    // KEY FIX: Escape literal newlines/tabs inside JSON string values
    // The LLM outputs actual \n chars inside "analysis" strings â†’ invalid JSON
    const escaped = escapeNewlinesInJsonStrings(jsonCandidate);
    try { const r = JSON.parse(escaped); r.date = date; return r; } catch { }

    // Extra repair: trailing commas, control chars
    const cleaned = escaped
        .replace(/,\s*}/g, '}')
        .replace(/,\s*]/g, ']');
    try { const r = JSON.parse(cleaned); r.date = date; return r; } catch (e: any) {
        console.error('[BRIEFING] JSON repair still failed:', e.message?.slice(0, 100));
    }

    // Last resort: markdown parser
    return extractBlogFromMarkdown(text, date);
}

/**
 * Escape literal newlines/tabs inside JSON string values.
 * Walks character-by-character tracking whether we're inside a string.
 */
function escapeNewlinesInJsonStrings(input: string): string {
    const out: string[] = [];
    let inString = false;
    let escape = false;

    for (let i = 0; i < input.length; i++) {
        const ch = input[i];

        if (escape) {
            out.push(ch);
            escape = false;
            continue;
        }

        if (ch === '\\' && inString) {
            out.push(ch);
            escape = true;
            continue;
        }

        if (ch === '"') {
            inString = !inString;
            out.push(ch);
            continue;
        }

        if (inString) {
            // Replace literal newlines/tabs/CRs inside strings with escape sequences
            if (ch === '\n') { out.push('\\n'); continue; }
            if (ch === '\r') { out.push('\\r'); continue; }
            if (ch === '\t') { out.push('\\t'); continue; }
        }

        out.push(ch);
    }

    return out.join('');
}

/** Parse a markdown-formatted blog response (when LLM ignores JSON format) */
function extractBlogFromMarkdown(text: string, date: string): BlogBriefing | null {
    if (!text || text.length < 100) return null;

    const lines = text.split('\n');
    const sections: BlogSection[] = [];
    let currentSection: BlogSection | null = null;
    let title = '';
    let intro = '';
    const insights: string[] = [];

    for (const line of lines) {
        // Extract title from H1
        if (line.startsWith('# ') && !title) {
            title = line.slice(2).trim();
            continue;
        }
        // Section headings (H2)
        if (line.startsWith('## ')) {
            if (currentSection) sections.push(currentSection);
            currentSection = {
                heading: line.slice(3).trim(),
                analysis: '',
                sources: [],
            };
            continue;
        }
        // Collect content into current section
        if (currentSection) {
            // Extract URLs as sources
            const urlMatch = line.match(/\[([^\]]+)\]\((https?:\/\/[^)]+)\)/);
            if (urlMatch) {
                currentSection.sources.push({ title: urlMatch[1], url: urlMatch[2] });
            }
            currentSection.analysis += line + '\n';
        } else if (!intro && line.trim()) {
            intro += line + ' ';
        }
    }
    if (currentSection) sections.push(currentSection);

    if (sections.length === 0) {
        // Treat entire text as one section
        sections.push({
            heading: 'ä»Šæ—¥ AI/Agent åŠ¨æ€åˆ†æ',
            analysis: text.slice(0, 3000),
            sources: [],
        });
    }

    return {
        title: title || `AI/Agent æ·±åº¦åˆ†æ â€” ${date}`,
        date,
        intro: intro.trim().slice(0, 200) || `ä»Šæ—¥ AI/Agent é¢†åŸŸæ·±åº¦åˆ†ææŠ¥å‘Šã€‚`,
        sections,
        conclusion: 'ä»¥ä¸Šä¸ºä»Šæ—¥ AI/Agent é¢†åŸŸæ ¸å¿ƒåŠ¨æ€çš„æ·±åº¦åˆ†æã€‚',
        key_insights: insights.length > 0 ? insights : ['è¯¦è§æ­£æ–‡åˆ†æ'],
    };
}

function generateFallbackBriefing(items: RawItem[], date: string): BlogBriefing {
    return {
        title: `AI/Agent æ¯æ—¥ç®€æŠ¥ â€” ${date}`,
        date,
        intro: `ä»Šæ—¥æ”¶é›†äº† ${items.length} æ¡ AI/Agent é¢†åŸŸåŠ¨æ€ã€‚`,
        sections: [{
            heading: 'ğŸ“Š ä»Šæ—¥åŠ¨æ€æ±‡æ€»',
            analysis: items.slice(0, 15).map(i =>
                `â€¢ **${i.title}** (${i.source}, â¬†${i.score})\n  ${i.summary}\n  ${i.url}`
            ).join('\n\n'),
            sources: items.slice(0, 15).map(i => ({ title: i.title, url: i.url })),
        }],
        conclusion: 'ç”±äº LLM åˆ†æä¸å¯ç”¨ï¼Œä»¥ä¸Šä¸ºåŸå§‹æ•°æ®æ±‡æ€»ã€‚',
        key_insights: ['LLM åˆ†ææš‚ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ API æ¥å…¥'],
    };
}

// â”€â”€â”€ Phase 4: Beautiful Feishu Document â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function createBlogDoc(blog: BlogBriefing): Promise<string> {
    const client = getFeishuClient();
    const accessToken = await getFeishuToken();

    // 1. Create document
    const createRes = await client.docx.document.create({
        data: { title: `ğŸ“° ${blog.title}`, folder_token: '' },
    });
    const docId = createRes.data?.document?.document_id;
    if (!docId) throw new Error('Failed to create Feishu document');

    // 2. Get root block
    const docBlock = await client.docx.documentBlock.list({
        path: { document_id: docId },
        params: { page_size: 1 },
    });
    const rootBlockId = docBlock.data?.items?.[0]?.block_id || docId;

    // 3. Build blocks with safe char limits
    const blocks: any[] = [];
    const MAX_TEXT_RUN = 450;  // Feishu text_run limit is ~500 chars

    // Helper: safe text block (auto-splits if too long)
    const makeText = (content: string) => ({
        block_type: 2,
        text: {
            elements: [{ text_run: { content: content.slice(0, MAX_TEXT_RUN) } }],
            style: {},
        },
    });

    // Helper: bold text block
    const makeBold = (content: string) => ({
        block_type: 2,
        text: {
            elements: [{ text_run: { content: content.slice(0, MAX_TEXT_RUN), text_element_style: { bold: true } } }],
            style: {},
        },
    });

    // Helper: heading-style
    const makeHeading = (content: string) => ({
        block_type: 2,
        text: {
            elements: [{
                text_run: {
                    content: `â–${content}`.slice(0, MAX_TEXT_RUN),
                    text_element_style: { bold: true },
                },
            }],
            style: {},
        },
    });

    // Helper: quote-like block
    const makeQuote = (content: string) => ({
        block_type: 2,
        text: {
            elements: [{
                text_run: {
                    content: `â”‚ ${content}`.slice(0, MAX_TEXT_RUN),
                    text_element_style: { italic: true },
                },
            }],
            style: {},
        },
    });

    // Helper: divider
    const makeDivider = () => ({
        block_type: 2,
        text: {
            elements: [{ text_run: { content: 'â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”' } }],
            style: {},
        },
    });

    // Helper: link
    const makeLink = (text: string, url: string) => ({
        block_type: 2,
        text: {
            elements: [{
                text_run: {
                    content: `ğŸ”— ${text}`.slice(0, MAX_TEXT_RUN),
                    text_element_style: {
                        link: { url },
                    },
                },
            }],
            style: {},
        },
    });

    // Helper: split long text into multiple blocks
    const pushLongText = (text: string) => {
        // Split by sentences (Chinese period, English period, newlines)
        const chunks: string[] = [];
        let current = '';
        const sentences = text.split(/(?<=[ã€‚ï¼ï¼Ÿ.!?\n])/g);
        for (const s of sentences) {
            if ((current + s).length > MAX_TEXT_RUN && current.length > 0) {
                chunks.push(current.trim());
                current = s;
            } else {
                current += s;
            }
        }
        if (current.trim()) chunks.push(current.trim());

        for (const chunk of chunks) {
            if (chunk.startsWith('>') || chunk.startsWith('> ')) {
                blocks.push(makeQuote(chunk.replace(/^>\s*/, '')));
            } else {
                blocks.push(makeText(chunk));
            }
        }
    };

    // â”€â”€ Build Document Structure â”€â”€

    // Date line
    blocks.push(makeText(`ğŸ“… ${blog.date}`));

    // Intro
    blocks.push(makeBold('ğŸ“Œ å¯¼è¯»'));
    pushLongText(blog.intro);
    blocks.push(makeDivider());

    // Sections
    for (const section of blog.sections ?? []) {
        blocks.push(makeHeading(section.heading));

        // Split analysis into paragraphs for readability
        const paragraphs = section.analysis.split('\n\n').filter(Boolean);
        for (const para of paragraphs) {
            pushLongText(para);
        }

        // Source links
        if (section.sources?.length > 0) {
            blocks.push(makeText('ğŸ“ å‚è€ƒæ¥æº:'));
            for (const src of section.sources.slice(0, 5)) {
                if (src.url) {
                    blocks.push(makeLink(src.title.slice(0, 55), src.url));
                }
            }
        }

        blocks.push(makeDivider());
    }

    // Key Insights
    if (blog.key_insights?.length > 0) {
        blocks.push(makeHeading('ğŸ’¡ ä»Šæ—¥æ ¸å¿ƒæ´å¯Ÿ'));
        for (let i = 0; i < blog.key_insights.length; i++) {
            blocks.push(makeBold(`${i + 1}. ${blog.key_insights[i]}`));
        }
        blocks.push(makeDivider());
    }

    // Conclusion
    blocks.push(makeHeading('ğŸ“ æ€»ç»“'));
    blocks.push(makeText(blog.conclusion));
    blocks.push(makeText(`\nâ€” FishbigAgent ğŸŸ è‡ªåŠ¨ç”Ÿæˆ`));

    // 4. Insert blocks in batches of 20 (Feishu limit)
    const BATCH_SIZE = 20;
    let totalInserted = 0;
    for (let i = 0; i < blocks.length; i += BATCH_SIZE) {
        const batch = blocks.slice(i, i + BATCH_SIZE);
        const url = `https://open.larksuite.com/open-apis/docx/v1/documents/${docId}/blocks/${rootBlockId}/children`;
        const res = await fetch(url, {
            method: 'POST',
            headers: {
                'Authorization': `Bearer ${accessToken}`,
                'Content-Type': 'application/json',
            },
            body: JSON.stringify({ children: batch, index: -1 }),
        });
        const resJson = await res.json() as { code?: number; msg?: string };
        if (resJson.code !== 0) {
            console.error(`[BRIEFING] Block batch ${Math.floor(i / BATCH_SIZE) + 1} error:`, resJson.code, resJson.msg);
            // Try inserting blocks one by one to find the bad one
            for (let j = 0; j < batch.length; j++) {
                const singleRes = await fetch(url, {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${accessToken}`,
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({ children: [batch[j]], index: -1 }),
                });
                const singleJson = await singleRes.json() as { code?: number; msg?: string };
                if (singleJson.code === 0) {
                    totalInserted++;
                } else {
                    console.error(`[BRIEFING] Block ${i + j} failed:`, JSON.stringify(batch[j]).slice(0, 200));
                }
            }
        } else {
            totalInserted += batch.length;
        }
    }
    console.log(`[BRIEFING] âœ… Inserted ${totalInserted}/${blocks.length} blocks into blog document`);

    const docUrl = `https://bytedance.larkoffice.com/docx/${docId}`;
    console.log(`[BRIEFING] Created blog: ${docUrl}`);
    return docUrl;
}

// â”€â”€â”€ Send Doc Link â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function sendDocLink(chatId: string, docUrl: string, title: string): Promise<void> {
    const client = getFeishuClient();
    await client.im.message.create({
        params: { receive_id_type: 'chat_id' },
        data: {
            receive_id: chatId,
            msg_type: 'interactive',
            content: JSON.stringify({
                config: { wide_screen_mode: true },
                header: { title: { tag: 'plain_text', content: title } },
                elements: [
                    {
                        tag: 'action',
                        actions: [{
                            tag: 'button',
                            text: { tag: 'plain_text', content: 'ğŸ“„ æ‰“å¼€æ·±åº¦åˆ†æ' },
                            url: docUrl,
                            type: 'primary',
                        }],
                    },
                ],
            }),
        },
    });
}

// â”€â”€â”€ Main Entry â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Generate the daily deep analysis briefing.
 * Pipeline: Collect â†’ Fetch â†’ Analyze â†’ Document
 */
export async function generateDailyBriefing(chatId: string): Promise<string> {
    const date = new Date().toISOString().slice(0, 10);
    console.log(`[BRIEFING] ğŸ“° Generating deep analysis briefing for ${date}...`);

    // Phase 1: Collect raw data
    const items = await collectAllData();
    if (items.length === 0) {
        console.error('[BRIEFING] No data collected');
        return '';
    }

    // Phase 2: Fetch full articles
    const enrichedItems = await enrichWithFullContent(items);

    // Phase 3: LLM deep analysis
    const blog = await generateBlogAnalysis(enrichedItems, date);

    // Phase 4: Create beautiful Feishu document
    const docUrl = await createBlogDoc(blog);

    // Send briefing to chat
    await sendDocLink(chatId, docUrl, `ğŸ“° ${blog.title}`);
    console.log(`[BRIEFING] âœ… Deep analysis sent to chat ${chatId}`);

    // Phase 5: Write materials to Notion FIRST (to get page IDs for linking)
    let materialMappings: { url: string; pageId: string; title: string }[] = [];
    try {
        const { writeMaterialsToNotion } = await import('../channels/notion-writer.js');
        materialMappings = await writeMaterialsToNotion(enrichedItems.map(i => ({
            title: i.title,
            source: i.source,
            url: i.url,
            summary: (i.summary || '').slice(0, 2000),
            score: i.score,
            date,
        })));
    } catch (e: any) {
        console.error('[BRIEFING] Notion materials write failed:', e.message);
    }

    // Phase 6: Generate content topic library (with material page IDs)
    try {
        const briefingText = enrichedItems.map(i =>
            `[${i.source}] ${i.title}\n${i.summary || ''}\n${i.fullContent ? i.fullContent.slice(0, 300) : ''}`
        ).join('\n---\n');
        await generateTopicLibrary(chatId, briefingText, enrichedItems, materialMappings);
    } catch (e: any) {
        console.error('[BRIEFING] Topic generation failed:', e.message);
    }

    // Phase 7: Write briefing to Notion
    try {
        const { writeBriefingToNotion } = await import('../channels/notion-writer.js');
        await writeBriefingToNotion({
            title: blog.title,
            date,
            sectionCount: blog.sections?.length ?? 0,
            feishuUrl: docUrl,
            sections: (blog.sections ?? []).map(s => ({
                heading: s.heading,
                analysis: s.analysis,
            })),
        });
    } catch (e: any) {
        console.error('[BRIEFING] Notion briefing write failed:', e.message);
    }

    return docUrl;
}
